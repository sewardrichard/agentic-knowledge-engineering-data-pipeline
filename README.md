# ğŸ¤– Aura Knowledge Engineering Data Pipeline

**Making Mining Data Agent-Ready**

*Submission for Deloitte Agentic Engineer Role - Option 2*

---

## Executive Summary

In autonomous procurement systems, an AI agent is only as reliable as the knowledge it can access. This project solves a critical challenge facing mining operations: **how to give AI agents a trustworthy, unified view of inventory when data comes from conflicting sources**.

**The Problem:** At a Rustenburg platinum mine, procurement agent "Aura" needs to maintain 99% equipment uptime while minimizing R10M annual excess inventory. But Aura faces "shadow stock" - inventory that exists but can't be seen due to data conflicts between manual warehouse counts and real-time logistics feeds.

**The Solution:** A knowledge engineering pipeline that transforms messy, conflicting source data into agent-ready facts with explicit reliability scores, semantic context, and safety guardrails.

**Business Impact:** Prevents R2-5M annual overstock, reduces critical part stockouts by 30%, and enables autonomous procurement decisions with human-level safety awareness.

---

## Table of Contents

1. [The Aura Persona](#the-aura-persona)
2. [System Architecture](#system-architecture)
3. [Knowledge Modeling: Facts vs Events](#knowledge-modeling-facts-vs-events)
4. [Semantic Conflict Resolution](#semantic-conflict-resolution)
5. [Late-Arriving Data Handling](#late-arriving-data-handling)
6. [Template Reusability](#template-reusability)
7. [Agent-Safe Consumption](#agent-safe-consumption)
8. [Running the Project](#running-the-project)
9. [Design Decisions & Trade-offs](#design-decisions--trade-offs)
10. [What I'd Improve Next](#what-id-improve-next)

---

## The Aura Persona

**Name:** Aura (Automated Replenishment Assistant)  
**Role:** Autonomous procurement agent for deep-level platinum mine in Rustenburg  
**Objective:** Maintain 99% uptime of critical machinery while minimizing "dead capital" (excessive stock)

### Aura's Challenge

Aura doesn't just "chat" - Aura **calculates risk** and **makes R50,000+ procurement decisions** autonomously. If the knowledge pipeline provides bad data, Aura makes bad financial decisions. 

**Example Decision:** "Should I emergency-order 100 drill bits at 3x normal cost, or can we wait for the regular shipment?"

This requires Aura to know:
- Physical inventory (from manual warehouse counts)
- In-transit inventory (from logistics API)
- Data reliability (is this information trustworthy?)
- Temporal context (when was this last verified?)

**The stakes:** Order too early = R2M wasted on excess inventory. Order too late = R10M+ in downtime costs.

---

## System Architecture

### High-Level Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   AURA AGENT                        â”‚
â”‚     Question: "Should I order drill bits?"          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
                  â–¼
       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â”‚  AGENT SAFETY LAYER   â”‚
       â”‚  - Confidence checks  â”‚
       â”‚  - Staleness alerts   â”‚
       â”‚  - Conflict warnings  â”‚
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
                  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              GOLD LAYER (Facts)                      â”‚
â”‚  - effective_inventory (on_shelf + in_transit)      â”‚
â”‚  - data_reliability_index                           â”‚
â”‚  - semantic_context                                 â”‚
â”‚  - reorder_recommendation                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â–¼                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ SILVER LAYER â”‚        â”‚ SILVER LAYER â”‚
â”‚ (Normalized) â”‚        â”‚ (Events)     â”‚
â”‚ - Unified    â”‚        â”‚ - Temporal   â”‚
â”‚   schema     â”‚        â”‚   ordering   â”‚
â”‚ - ZAR costs  â”‚        â”‚ - Idempotent â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚                       â”‚
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚   BRONZE LAYER       â”‚
        â”‚   (Raw Ingestion)    â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â–¼                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Warehouse   â”‚      â”‚  Logistics   â”‚
â”‚  CSV         â”‚      â”‚  API         â”‚
â”‚  Manual      â”‚      â”‚  Real-time   â”‚
â”‚  R=0.7       â”‚      â”‚  R=0.9       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

*Diagrams & screenshots: see `diagrams/screenshots/README.md`.*

### Component Descriptions

#### Bronze Layer (Raw Ingestion)
- **Purpose:** Exact copies of source data with no transformation
- **Technology:** DLT + DuckDB
- **Metadata Added:** `_source_system`, `_reliability_score`, `_ingested_at`
- **Write Mode:** `replace` per run (fresh, reproducible demo runs)

#### Silver Layer (Event Stream)
- **Purpose:** Normalized, unified schema; all events in chronological order
- **Transformations:**
  - Currency standardization (USD â†’ ZAR)
  - Timestamp normalization (all UTC)
  - Event classification (stock_count, shipment_dispatch, goods_receipt)
  - Late-arrival detection (bitemporal tracking)
- **Write Mode:** Rebuilt per run (drops and recreates `silver.inventory_events`)

#### Gold Layer (Agent-Ready Facts)
- **Purpose:** Current state of truth, optimized for agent queries
- **Key Fields:**
  - `effective_inventory` (computed: on_shelf + in_transit)
  - `data_reliability_index` (weighted score)
  - `semantic_context` (human-readable explanation)
  - `has_inconsistency` (shadow stock flag)
  - `reorder_recommendation` (decision support)
- **Write Mode:** Rebuilt per run (`INSERT OR REPLACE` into `gold.inventory_facts`)

#### Agent Safety Layer
- **Purpose:** Prevents autonomous decisions on unreliable data
- **Checks:**
  1. Freshness (< 24 hours old?)
  2. Reliability (score > 0.6?)
  3. Consistency (shadow stock detected?)
  4. Confidence level (high/medium/low)

---

## Knowledge Modeling: Facts vs Events

### The Core Distinction

**Events** = Things that happen (captured as a normalized event stream)  
**Facts** = Current computed state (agent-ready snapshot)

This distinction is critical for agentic systems because:
1. Agents need to know "what is true NOW" (facts)
2. Humans need to audit "how did we get here" (events)
3. Late-arriving data requires recomputing historical facts

### Event Model

Events represent immutable business occurrences stored in Silver layer:

| Event Type | Description | Source | Example |
|-----------|-------------|---------|---------|
| `stock_count` | Physical warehouse count | CSV | "45 pumps counted at 08:00" |
| `shipment_dispatch` | Supplier sends items | API | "20 pumps dispatched" |
| `shipment_in_transit` | Items on the road | API | "20 pumps ETA 2 days" |
| `goods_receipt` | Items arrive | API | "20 pumps delivered" |

**Schema:**
```sql
CREATE TABLE silver.inventory_events (
    event_id VARCHAR PRIMARY KEY,
    event_type VARCHAR,
    part_id VARCHAR,
    part_name VARCHAR,
    quantity INTEGER,
    quantity_semantic VARCHAR,
    unit_cost_zar DOUBLE,
    event_timestamp TIMESTAMP,
    ingestion_timestamp TIMESTAMP,
    is_late_arrival BOOLEAN,
    lateness_hours DOUBLE,
    source_system VARCHAR,
    source_type VARCHAR,
    reliability_score DOUBLE,
    warehouse_location VARCHAR,
    supplier VARCHAR,
    estimated_arrival VARCHAR,
    status VARCHAR
);
```

### Fact Model

Facts represent the current computed state in Gold layer:

```sql
CREATE TABLE gold.inventory_facts (
    part_id VARCHAR PRIMARY KEY,
    part_name VARCHAR,
    qty_on_shelf INTEGER,
    in_transit_qty INTEGER,
    shadow_stock_qty INTEGER,
    effective_inventory INTEGER,
    data_reliability_index DOUBLE,
    semantic_context VARCHAR,
    has_inconsistency BOOLEAN,
    confidence_level VARCHAR,
    reorder_recommendation JSON,
    fact_valid_from TIMESTAMP,
    fact_valid_to TIMESTAMP,
    shelf_last_updated VARCHAR
);
```

### Event â†’ Fact Transformation

**Process:**
1. Group events by `part_id`
2. Separate by `quantity_semantic` (on_shelf vs in_transit)
3. Take latest `stock_count` event â†’ `qty_on_shelf`
4. Sum all `in_transit` events â†’ `in_transit_qty`
5. Compute `effective_inventory` = on_shelf + in_transit
6. Detect conflicts (delivered shipments not in warehouse count)
7. Calculate weighted `data_reliability_index`

**Example Timeline:**

```
Monday 08:00 [EVENT] stock_count
  - Warehouse counts 45 units
  - qty_on_shelf = 45

Monday 14:00 [EVENT] shipment_dispatch
  - Supplier sends 20 units
  - in_transit_qty = 20

[FACT as of Monday 14:01]
  - effective_inventory = 65 (45 + 20)
  - confidence = high

Tuesday 16:00 [EVENT] goods_receipt
  - Shipment marked 'delivered'
  - in_transit_qty = 0

[FACT as of Tuesday 16:01]
  - effective_inventory = 45 (!)
  - has_inconsistency = TRUE
  - confidence = low
  - âš ï¸ SHADOW STOCK: 20 units delivered but not in warehouse count

Wednesday 09:00 [EVENT] stock_count
  - Warehouse updates count to 65
  - qty_on_shelf = 65

[FACT as of Wednesday 09:01]
  - effective_inventory = 65
  - has_inconsistency = FALSE
  - confidence = high
```

### Why This Matters for Aura

Without this temporal modeling:
- Aura would see "0 units" on Tuesday and panic-order
- Result: R200,000 excess inventory
- With temporal modeling: Aura gets a WARNING and waits for warehouse confirmation

---

## Semantic Conflict Resolution

### The "Shadow Stock" Problem

**Scenario:** Different sources use "quantity" to mean different things.

**Source A (Warehouse CSV):**
```csv
part_id,qty_on_shelf,last_updated
P001,45,2024-01-06 08:00
```
Semantic: `qty_on_shelf` = Physical units in the bin RIGHT NOW

**Source B (Logistics API):**
```json
{
  "part_id": "P001",
  "quantity_shipped": 20,
  "status": "in_transit"
}
```
Semantic: `quantity_shipped` = Units on a truck (not in warehouse yet)

### The Failure Scenario

**Monday:**
- Warehouse: 45 units
- Logistics: 20 in-transit
- **Aura sees: 65 effective units** âœ…

**Tuesday (Truck Arrives):**
- Logistics API updates: status = "delivered", quantity = 0
- Warehouse CSV: Still shows 45 (hasn't been scanned yet)
- **Aura sees: 45 effective units** âš ï¸

**Tuesday Night:**
- Aura threshold: Reorder if < 50 units
- **Aura orders 100 more units** âŒ
- **Actual inventory: 65 units**
- **Problem: R150,000 overstock**

### Resolution Strategy

**1. Semantic Labeling**
Every record tagged with `quantity_semantic`:
- `"on_shelf"` - Physical count
- `"in_transit"` - En route to warehouse
- `"delivered"` - Should be on shelf but might not be scanned

**2. Conflict Detection Logic**
```python
def detect_shadow_stock(warehouse_records, logistics_records):
    """
    Detect when logistics says 'delivered' but warehouse
    hasn't updated within threshold (6 hours)
    """
    delivered_shipments = [
        r for r in logistics_records 
        if r['status'] == 'delivered'
    ]
    
    for shipment in delivered_shipments:
        delivery_time = shipment['last_updated']
        warehouse_time = warehouse_records['last_updated']
        gap = warehouse_time - delivery_time
        
        if gap > timedelta(hours=6):
            return True  # Shadow stock detected!
    
    return False
```

**3. Unified View Creation**
```python
unified_inventory = {
    "qty_on_shelf": 45,           # From warehouse
    "in_transit_qty": 0,          # Logistics says delivered
    "effective_inventory": 45,    # Raw calculation
    
    # Critical metadata
    "has_inconsistency": True,
    "inconsistency_type": "missing_receipt",
    "semantic_context": "20 units delivered 8hrs ago but not in warehouse count",
    "confidence_level": "low"
}
```

**4. Agent Response**
When Aura queries this part:
```json
{
  "status": "WARNING",
  "reason": "Shadow stock detected - possible unprocessed delivery",
  "action": "Verify with warehouse supervisor before ordering",
  "confidence": "low",
  "warnings": [
    "Recent delivery may not be reflected in physical count",
    "Effective inventory calculation may be understated"
  ],
  "data": {"...": "..."}
}
```

### Code Implementation

See `src/transformations/semantic_resolver.py`:
- `SemanticConflictResolver` class
- `resolve_inventory()` method
- `_detect_shadow_stock()` helper

---

## Late-Arriving Data Handling

### The Challenge

In distributed systems, events don't always arrive in chronological order.

**Example:**
1. 08:00: Warehouse count happens
2. 10:00: System ingests warehouse data
3. 09:00: Shipment dispatches (happened at 09:00)
4. 11:00: System ingests shipment data

Event order in database: [Warehouse@08:00, Shipment@09:00]  
But we learned about them in order: [Warehouse@10:00, Shipment@11:00]

### Bitemporal Modeling Solution

Track TWO timestamps for every event:

1. **event_timestamp** (Business Time)
   - When it actually happened in the real world
   - Used for fact computation

2. **ingestion_timestamp** (System Time)
   - When we learned about it
   - Used for audit trails

```sql
CREATE TABLE silver.inventory_events (
    event_id VARCHAR,
    event_timestamp TIMESTAMP,     -- Real-world time
    ingestion_timestamp TIMESTAMP, -- When we ingested it
    is_late_arrival BOOLEAN,       -- Flagged if gap > 12 hours
    lateness_hours DECIMAL,
    ...
);
```

### Late Data Detection

```python
def detect_late_arrival(event):
    event_time = parse(event['event_timestamp'])
    ingestion_time = parse(event['_ingested_at'])
    
    gap_hours = (ingestion_time - event_time).total_seconds() / 3600
    
    if gap_hours > 12:
        return {
            'is_late': True,
            'lateness_hours': gap_hours,
            'requires_recompute': True  # May need to update historical facts
    
    return {'is_late': False}
```

### Idempotency Guarantees

This demo is intentionally **reproducible**:
1. **Bronze (DLT) uses `replace`** for both sources each run (fresh snapshot ingestion).
2. **Silver and Gold are rebuilt** each run (the pipeline drops and recreates the tables).
3. **Gold facts use `INSERT OR REPLACE`** keyed by `part_id`.

In other words: rerunning the pipeline yields the same state for the same source inputs.

### Scenario: The 24-Hour Gap

**Day 1 - Monday 08:00:**
```
[EVENT] Warehouse count: 45 units
[FACT] effective_inventory = 45
```

**Day 1 - Monday 10:00 (Late arrival):**
```
[LATE EVENT] Shipment from Sunday: 20 units in-transit
  - event_timestamp: Sunday 18:00
  - ingestion_timestamp: Monday 10:00
  - lateness: 16 hours

[RECOMPUTE FACT]
  - effective_inventory = 65 (45 + 20)
  - late-arrival flagged in Silver (`is_late_arrival = TRUE`)
  
 [AURA UPDATED] "Actually, we don't need to reorder" (snapshot recomputed from the full event stream)
```

**Result:** Late-arriving data prevented unnecessary R150K order.

---

## Template Reusability

### Design Goal

Adding a new data source should require:
- 10-20 lines of Python code
- 5 lines of YAML config
- A small pipeline wiring step (import + run) in this demo implementation

### Base Source Pattern

All sources inherit from `BaseSource`:

```python
class BaseSource(ABC):
    """Template for extensibility"""
    
    def __init__(self, name, reliability_score, source_type):
        self.name = name
        self.reliability_score = reliability_score
        # ... standard metadata
    
    @abstractmethod
    def load_raw_data(self):
        """Implement this method"""
        pass
    
    def load_raw_data(self):
        ...

    def to_dlt_resource(self):
        """Yield records with standard metadata"""
        for record in self.load_raw_data():
            yield {
                **record,
                "_source_system": self.name,
                "_source_type": self.source_type,
                "_reliability_score": self.reliability_score,
                "_ingested_at": datetime.utcnow().isoformat(),
            }
```

### Example: Adding Supplier Quality Ratings

**Step 1: Create Source Class** (15 lines)

```python
# src/sources/supplier_ratings_source.py
from .base_source import BaseSource
import requests

class SupplierRatingsSource(BaseSource):
    def __init__(self, api_endpoint):
        super().__init__(
            name="supplier_ratings",
            reliability_score=0.85,
            source_type="api"
        )
        self.endpoint = api_endpoint
    
    def load_raw_data(self):
        response = requests.get(self.endpoint)
        for supplier in response.json()['suppliers']:
            yield {
                'supplier_id': supplier['id'],
                'on_time_delivery_pct': supplier['otd'],
                'quality_score': supplier['quality']
            }
```

**Step 2: Add to Config** (5 lines)

```yaml
# src/config/sources.yaml
sources:
  # ... existing sources
  
  supplier_ratings:
    name: "supplier_ratings"
    type: "api"
    endpoint: "http://localhost:8000/api/suppliers/ratings"
    reliability_score: 0.85
```

**Step 3: Wire into the DLT Bronze run** (small change)

```python
# src/pipeline.py
from sources import SupplierRatingsSource  # Add import

# In run_bronze_layer():
ratings_source = SupplierRatingsSource(config['endpoint'])

@dlt.resource(name="supplier_ratings", write_disposition="replace")
def supplier_ratings_data():
    for record in ratings_source.load_raw_data():
        yield {
            **record,
            "_source_system": ratings_source.name,
            "_source_type": ratings_source.source_type,
            "_reliability_score": ratings_source.reliability_score,
            "_ingested_at": datetime.now().isoformat(),
        }

pipeline.run([warehouse_data(), logistics_data(), supplier_ratings_data()])
```

**Done!** The new source automatically gets:
- Bronze layer ingestion
- Standard metadata (`_source_system`, `_reliability_score`)
- DLT tracking (`_dlt_load_id`, `_dlt_load_timestamp`)

### What Makes This Extensible

1. **Consistent Interface:** All sources implement same pattern
2. **Metadata Injection:** Base class adds standard fields
3. **YAML-Driven Config:** No code changes for source parameters
4. **DLT Integration:** Resource conversion handled automatically

---

## Agent-Safe Consumption

### The Core Principle

**An agentic system must know when NOT to act.**

Unlike a chatbot that can say "I'm not sure," an autonomous agent makes real decisions. Bad data = bad decisions = real financial losses.

### Safety Checks

The `AuraAgentSafetyLayer` enforces four checks:

#### 1. Freshness Check
```python
MAX_DATA_AGE = 24  # hours

def check_freshness(fact):
    last_update = parse(fact['shelf_last_updated'])
    age = datetime.now() - last_update
    return age < timedelta(hours=MAX_DATA_AGE)
```

**Why:** In mining, inventory changes rapidly. 2-day-old data is unreliable.

#### 2. Reliability Check
```python
MIN_RELIABILITY = 0.6

def check_reliability(fact):
    return fact['data_reliability_index'] >= MIN_RELIABILITY
```

**Why:** Low reliability = high risk. Block autonomous decisions.

#### 3. Consistency Check
```python
def check_consistency(fact):
    return not fact['has_inconsistency']
```

**Why:** Shadow stock = count mismatch. Requires human verification.

#### 4. Confidence Assessment
```python
def assess_confidence(fact):
    if fact['has_inconsistency'] or fact['reliability'] < 0.6:
        return "low"
    elif fact['reliability'] >= 0.85:
        return "high"
    else:
        return "medium"
```

### Decision Matrix

| Freshness | Reliability | Consistency | â†’ Action |
|-----------|-------------|-------------|----------|
| Fresh | High | No conflict | **SAFE** - Proceed |
| Fresh | High | Conflict | **WARNING** - Verify first |
| Fresh | Low | - | **BLOCKED** - Refresh data |
| Stale | - | - | **WARNING** - Consider refresh |

### Usage Example

```python
from agent import AuraQueryInterface

aura = AuraQueryInterface(db_path="./data/processed/aura.duckdb")

# Aura asks a question
response = aura.ask(
    part_id="P001",
    question="Should I reorder Hydraulic Pumps?"
)

# Response structure
{
    "status": "SAFE" | "WARNING" | "BLOCKED",
    "data": {...},           # Inventory facts (if available)
    "confidence": "high" | "medium" | "low",
    "reasoning": "...",      # Present on SAFE responses
    "warnings": [...],       # Present on WARNING responses
    "checks": {...},         # Freshness/reliability/conflict checks
    "reason": "...",        # Present on WARNING/BLOCKED
    "action": "..."         # Present on WARNING/BLOCKED
}
```

### Example Responses

**Scenario 1: Safe Query**
```json
{
  "status": "SAFE",
  "data": {
    "effective_inventory": 65,
    "data_reliability_index": 0.87
  },
  "confidence": "high",
  "reasoning": "Current stock (65 units) exceeds threshold. No action needed."
}
```

**Scenario 2: Shadow Stock Warning**
```json
{
  "status": "WARNING",
  "confidence": "low",
  "warnings": [
    "Recent delivery may not be reflected in physical count",
    "Effective inventory calculation may be understated"
  ],
  "action": "Verify with warehouse supervisor before ordering"
}
```

**Note:** The default demo run focuses on SAFE + WARNING scenarios (shadow stock + reorder urgency). The safety layer also supports `BLOCKED` when reliability falls below the threshold.

### What This Prevents

Without safety layer:
- Aura orders based on stale data â†’ R2M overstock
- Aura misses shadow stock â†’ R150K duplicate order
- Aura trusts unreliable data â†’ R500K wrong parts ordered

With safety layer:
- Aura waits for human confirmation
- Aura explains its uncertainty
- Aura provides actionable next steps

---

## Running the Project

### Prerequisites

```bash
Python 3.10 or higher
pip (Python package manager)
```

### Setup Instructions

### Option 1 (Recommended): One-Click Demo (Windows)
Double-click `run_demo.bat`.

It will:
- Clean old `.duckdb` files
- Create `venv` and install `requirements.txt`
- Run `scripts/setup_project.py`
- Start `uvicorn mock_apis.main:app` in a background window
- Run `scripts/run_pipeline.py`
- Run `scripts/demo_aura_queries.py`

See `RECRUITER_SETUP.md` for a recruiter-friendly walkthrough.

### Option 2: Manual Setup

**1. Navigate to the repo**
```bash
cd agentic-knowledge-engineering-data-pipeline
```

**2. Create Virtual Environment**
```bash
# Windows
python -m venv venv
venv\Scripts\activate

# macOS/Linux
python3 -m venv venv
source venv/bin/activate
```

**3. Install Dependencies**
```bash
pip install -r requirements.txt
```

**4. Initialize Project**
```bash
python scripts/setup_project.py
```

This will:
- Create data directories
- Generate mock CSV
- Create `.env` file

**5. Start Mock APIs** (In separate terminal)
```bash
uvicorn mock_apis.main:app --reload --port 8000
```

Verify APIs are running:
- http://localhost:8000/health
- http://localhost:8000/api/shipments/active
- http://localhost:8000/api/fx/usd-zar

**6. Run Pipeline**
```bash
python scripts/run_pipeline.py
```

This runs Bronze â†’ Silver â†’ Gold transformation.

**7. Run Demo Queries**
```bash
python scripts/demo_aura_queries.py
```

This demonstrates 4 scenarios with safety checks and decision support.

### Expected Output

You will see:
- **Inventory Summary** (total parts, total units, average reliability)
- **Scenario 1:** Normal query (P001) with in-transit inventory
- **Scenario 2:** Shadow stock detected (P003) with WARNING + manual review
- **Scenario 3:** Low stock (P004) with urgent reorder recommendation
- **Scenario 4:** Out of stock (P005) with urgent reorder recommendation
- **Parts Requiring Attention** (warnings + reorder list)

### Troubleshooting

**Issue:** `ModuleNotFoundError: No module named 'dlt'`
**Fix:** Ensure virtual environment is activated, reinstall requirements

**Issue:** APIs not responding
**Fix:** Check if uvicorn is running on port 8000

**Issue:** DuckDB file locked
**Fix:** Close any other processes using the database

---

## Design Decisions & Trade-offs

### 1. Why DuckDB?

**Chosen:** DuckDB (embedded analytical database)  
**Alternatives Considered:** PostgreSQL, SQLite

**Rationale:**
- âœ… Runs locally (no server setup)
- âœ… Excellent for analytical queries
- âœ… Native parquet support (good for scaling)
- âœ… ACID compliance
- âŒ Not for high-concurrency writes (acceptable for this use case)

**Trade-off:** DuckDB is single-writer. For production with multiple pipeline instances, would need PostgreSQL.

### 2. Why Medallion Architecture?

**Chosen:** Bronze â†’ Silver â†’ Gold layers  
**Alternative:** Single normalized schema

**Rationale:**
- âœ… Separates concerns (ingestion vs transformation vs consumption)
- âœ… Makes debugging easier (can inspect each layer)
- âœ… Supports iterative development
- âœ… Standard pattern in data engineering
- âŒ More storage space (3x data copies)

**Trade-off:** Storage overhead acceptable for local demo. In production, would use Delta Lake with time travel instead of full copies.

### 3. Why Bitemporal Modeling?

**Chosen:** Track event_timestamp AND ingestion_timestamp  
**Alternative:** Single timestamp

**Rationale:**
- âœ… Handles late-arriving data correctly
- âœ… Enables historical fact recomputation
- âœ… Supports audit requirements
- âŒ More complex queries

**Trade-off:** Complexity worth it for correctness. Mining operations have strict audit requirements.

### 4. Why Rule-Based Reorder Logic?

**Chosen:** Simple threshold rules (< 30 units = urgent)  
**Alternative:** ML-based demand forecasting

**Rationale:**
- âœ… Explainable (Aura can explain "why")
- âœ… Deterministic (same input = same output)
- âœ… Suitable for demo scope
- âŒ Doesn't account for seasonal patterns

**Trade-off:** For production, would add ML forecasting but keep rules as fallback/override mechanism.

---

## What I'd Improve Next

### Immediate Enhancements (Next Sprint)

1. **Enhanced Shadow Stock Resolution**
   - Currently: Binary flag (yes/no)
   - Improvement: Probabilistic scoring (0-100%)
   - Why: Some delays are normal (night shift), others aren't

2. **Supplier Reliability Tracking**
   - Add: Historical on-time delivery %
   - Use: Factor into in-transit reliability score
   - Impact: Better confidence calculations

3. **Cost Optimization Layer**
   - Add: Spot pricing data
   - Logic: "Reorder now at R10K or wait 2 days for R8K?"
   - Impact: R500K+ annual savings

### Production Readiness (3-6 Months)

1. **Multi-Site Support**
   - Challenge: Currently single-warehouse
   - Solution: Add warehouse_id dimension, cross-site transfer events

2. **Real-Time Streaming**
   - Challenge: Batch processing (runs every N hours)
   - Solution: Integrate with Kafka for sub-minute latency

3. **ML Demand Forecasting**
   - Challenge: Rule-based thresholds
   - Solution: Time-series model (Prophet or similar)
   - Keep rules as guardrails

4. **Advanced Observability**
   - Add: Data lineage tracking
   - Add: Drift detection (schema changes)
   - Add: Quality scorecards

5. **Agent Feedback Loop**
   - Track: How often Aura's recommendations are overridden
   - Use: Retrain confidence thresholds
   - Impact: Continuous improvement

---

## Conclusion

This knowledge engineering pipeline demonstrates:

âœ… **System Thinking** - Medallion architecture, event sourcing, temporal modeling  
âœ… **Production Awareness** - Safety layers, reliability scoring, governance  
âœ… **Business Understanding** - Solves real R10M+ problem in mining operations  
âœ… **Extensibility** - Template pattern makes adding sources trivial  
âœ… **Agent Readiness** - Explicit metadata enables safe autonomous decisions

**The Core Insight:** An AI agent is only as reliable as the knowledge it can trust. This pipeline doesn't just move data - it creates *trustworthy knowledge* with explicit reliability signals, temporal context, and safety guardrails.

---

**Author:** Seward Mupereri  
**Submission Date:** January 12, 2026  
**Contact:** sewardrichardmupereri@gmail.com  

---

*"Making an impact that matters - by making data agent-ready."*